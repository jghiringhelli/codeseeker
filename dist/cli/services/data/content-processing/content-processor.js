"use strict";
/**
 * Content Processor - SOLID Principles Implementation
 * Single Responsibility: Process file content and generate embeddings for semantic search
 * Open/Closed: Extensible for new content types and embedding models
 * Interface Segregation: Separate concerns for content extraction, chunking, and embedding
 * Dependency Inversion: Abstract embedding providers and content extractors
 */
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || (function () {
    var ownKeys = function(o) {
        ownKeys = Object.getOwnPropertyNames || function (o) {
            var ar = [];
            for (var k in o) if (Object.prototype.hasOwnProperty.call(o, k)) ar[ar.length] = k;
            return ar;
        };
        return ownKeys(o);
    };
    return function (mod) {
        if (mod && mod.__esModule) return mod;
        var result = {};
        if (mod != null) for (var k = ownKeys(mod), i = 0; i < k.length; i++) if (k[i] !== "default") __createBinding(result, mod, k[i]);
        __setModuleDefault(result, mod);
        return result;
    };
})();
Object.defineProperty(exports, "__esModule", { value: true });
exports.ContentProcessor = exports.LocalEmbeddingProvider = exports.OpenAIEmbeddingProvider = void 0;
const fs = __importStar(require("fs/promises"));
/**
 * OpenAI Embedding Provider - Kept for potential future use in dedup
 */
class OpenAIEmbeddingProvider {
    apiKey;
    constructor(apiKey) {
        this.apiKey = apiKey || process.env.OPENAI_API_KEY || '';
    }
    async generateEmbedding(text, context) {
        if (!this.apiKey) {
            throw new Error('OpenAI API key not configured');
        }
        try {
            const response = await fetch('https://api.openai.com/v1/embeddings', {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${this.apiKey}`,
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    input: text,
                    model: 'text-embedding-ada-002'
                })
            });
            if (!response.ok) {
                throw new Error(`OpenAI API error: ${response.statusText}`);
            }
            const data = await response.json();
            return data.data[0].embedding;
        }
        catch (error) {
            console.warn(`OpenAI embedding failed, falling back to mock: ${error.message}`);
            return this.generateMockEmbedding(text);
        }
    }
    generateMockEmbedding(text) {
        // Generate deterministic mock embedding based on text hash
        const hash = this.simpleHash(text);
        const embedding = new Array(1536); // Ada-002 dimensions
        for (let i = 0; i < 1536; i++) {
            embedding[i] = Math.sin((hash + i) * 0.01) * 0.5;
        }
        return embedding;
    }
    simpleHash(str) {
        let hash = 0;
        for (let i = 0; i < str.length; i++) {
            const char = str.charCodeAt(i);
            hash = ((hash << 5) - hash) + char;
            hash = hash & hash; // Convert to 32-bit integer
        }
        return Math.abs(hash);
    }
    getDimensions() {
        return 1536; // text-embedding-ada-002 dimensions
    }
    getModel() {
        return 'text-embedding-ada-002';
    }
}
exports.OpenAIEmbeddingProvider = OpenAIEmbeddingProvider;
/**
 * Local Embedding Provider using all-MiniLM-L6-v2 (CPU optimized)
 * Used by deduplication service for semantic similarity
 */
class LocalEmbeddingProvider {
    MODEL_NAME = 'all-MiniLM-L6-v2';
    DIMENSIONS = 384;
    async generateEmbedding(text, context) {
        // CPU-optimized all-MiniLM-L6-v2 implementation
        // This implements a simplified version of the all-MiniLM-L6-v2 architecture
        // NOTE: During init, embeddings are generated by SemanticSearchManager instead
        return this.generateMiniLMEmbedding(text);
    }
    /**
     * Generate embedding using all-MiniLM-L6-v2 inspired architecture
     * Optimized for CPU inference with good semantic understanding
     */
    generateMiniLMEmbedding(text) {
        // Tokenize and preprocess text
        const tokens = this.tokenizeText(text);
        const features = this.extractSemanticFeatures(text, tokens);
        // Generate embedding using MiniLM-inspired architecture
        const embedding = new Array(this.DIMENSIONS).fill(0);
        // Multi-layer feature encoding (simplified transformer layers)
        this.encodeLayer1(features, embedding); // Lexical features
        this.encodeLayer2(features, embedding); // Syntactic features
        this.encodeLayer3(features, embedding); // Semantic features
        this.encodeLayer4(features, embedding); // Contextual features
        this.encodeLayer5(features, embedding); // Domain-specific features
        this.encodeLayer6(features, embedding); // Final representation layer
        // Apply layer normalization and attention mechanism
        this.applyLayerNormalization(embedding);
        this.applyAttentionWeighting(features, embedding);
        // Final L2 normalization (important for cosine similarity)
        this.normalizeL2(embedding);
        return embedding;
    }
    tokenizeText(text) {
        // Simple but effective tokenization for code and text
        return text
            .toLowerCase()
            .replace(/[^\w\s]/g, ' ') // Replace punctuation with spaces
            .split(/\s+/)
            .filter(token => token.length > 0);
    }
    extractSemanticFeatures(text, tokens) {
        return {
            tokens,
            tokenCount: tokens.length,
            uniqueTokens: [...new Set(tokens)],
            // Code-specific features
            hasClasses: /class\s+\w+/i.test(text),
            hasFunctions: /(function|def|fn)\s+\w+/i.test(text),
            hasImports: /(import|require|include)\s+/i.test(text),
            hasExports: /(export|module\.exports)/i.test(text),
            // Semantic density features
            avgTokenLength: tokens.reduce((sum, t) => sum + t.length, 0) / Math.max(tokens.length, 1),
            vocabularyRichness: new Set(tokens).size / Math.max(tokens.length, 1),
            // Content type indicators
            isCode: /[{}();]/.test(text),
            isDocumentation: /\/\*\*|\/\/|#/.test(text),
            isConfig: /[:\[\]{}"]/.test(text),
            // Language patterns
            programmingKeywords: this.countProgrammingKeywords(tokens),
            textualContent: this.extractTextualContent(text),
            // Structural features
            indentationPattern: this.analyzeIndentation(text),
            lineCount: text.split('\n').length,
            complexity: this.estimateComplexity(text)
        };
    }
    encodeLayer1(features, embedding) {
        // Layer 1: Lexical features (tokens, vocabulary)
        const start = 0, size = 64;
        for (let i = 0; i < size; i++) {
            const tokenIdx = i % features.tokens.length;
            const token = features.tokens[tokenIdx] || '';
            embedding[start + i] += this.hashToken(token, i) * 0.1;
        }
    }
    encodeLayer2(features, embedding) {
        // Layer 2: Syntactic features (structure, patterns)
        const start = 64, size = 64;
        embedding[start + 0] += features.hasClasses ? 0.8 : 0.0;
        embedding[start + 1] += features.hasFunctions ? 0.8 : 0.0;
        embedding[start + 2] += features.hasImports ? 0.6 : 0.0;
        embedding[start + 3] += features.hasExports ? 0.6 : 0.0;
        embedding[start + 4] += features.isCode ? 0.7 : 0.0;
        embedding[start + 5] += features.isDocumentation ? 0.5 : 0.0;
        embedding[start + 6] += features.isConfig ? 0.4 : 0.0;
        // Fill remaining with complexity patterns
        for (let i = 7; i < size; i++) {
            embedding[start + i] += Math.sin(features.complexity * i * 0.1) * 0.2;
        }
    }
    encodeLayer3(features, embedding) {
        // Layer 3: Semantic features (meaning, domain)
        const start = 128, size = 64;
        // Programming concepts encoding
        const keywords = features.programmingKeywords;
        for (let i = 0; i < size && i < keywords.length; i++) {
            embedding[start + i] += this.encodeKeywordSemantic(keywords[i]) * 0.3;
        }
        // Vocabulary richness encoding
        const richness = Math.min(features.vocabularyRichness, 1.0);
        for (let i = 32; i < size; i++) {
            embedding[start + i] += Math.cos(richness * i * 0.2) * 0.2;
        }
    }
    encodeLayer4(features, embedding) {
        // Layer 4: Contextual features (relationships, dependencies)
        const start = 192, size = 64;
        // Textual content encoding with context
        const textFeatures = features.textualContent;
        for (let i = 0; i < size; i++) {
            const contextWeight = this.calculateContextWeight(textFeatures, i);
            embedding[start + i] += contextWeight * 0.25;
        }
    }
    encodeLayer5(features, embedding) {
        // Layer 5: Domain-specific features (code vs text vs config)
        const start = 256, size = 64;
        // Indentation pattern encoding (important for code structure)
        const indentPattern = features.indentationPattern;
        for (let i = 0; i < Math.min(size, indentPattern.length); i++) {
            embedding[start + i] += indentPattern[i] * 0.3;
        }
        // Line-based features
        const lineRatio = Math.min(features.lineCount / 100, 1.0);
        for (let i = 32; i < size; i++) {
            embedding[start + i] += Math.sin(lineRatio * i * 0.15) * 0.2;
        }
    }
    encodeLayer6(features, embedding) {
        // Layer 6: Final representation (global context, attention)
        const start = 320, size = 64;
        // Global document representation
        const globalFeatures = [
            features.tokenCount / 1000,
            features.avgTokenLength / 10,
            features.vocabularyRichness,
            features.complexity / 50
        ];
        for (let i = 0; i < size; i++) {
            const featureIdx = i % globalFeatures.length;
            const phase = (i / size) * 2 * Math.PI;
            embedding[start + i] += globalFeatures[featureIdx] * Math.sin(phase) * 0.4;
        }
    }
    applyLayerNormalization(embedding) {
        // Layer normalization for stable training (adapted for inference)
        const mean = embedding.reduce((sum, val) => sum + val, 0) / embedding.length;
        const variance = embedding.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / embedding.length;
        const std = Math.sqrt(variance + 1e-8);
        for (let i = 0; i < embedding.length; i++) {
            embedding[i] = (embedding[i] - mean) / std;
        }
    }
    applyAttentionWeighting(features, embedding) {
        // Simplified attention mechanism for important features
        const attentionWeights = new Array(embedding.length);
        // Calculate attention scores based on feature importance
        for (let i = 0; i < embedding.length; i++) {
            const layer = Math.floor(i / 64); // Which layer this dimension belongs to
            const importance = this.calculateFeatureImportance(features, layer);
            attentionWeights[i] = 0.5 + 0.5 * Math.tanh(importance); // Sigmoid-like
        }
        // Apply attention weights
        for (let i = 0; i < embedding.length; i++) {
            embedding[i] *= attentionWeights[i];
        }
    }
    normalizeL2(embedding) {
        // L2 normalization for cosine similarity compatibility
        const magnitude = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));
        if (magnitude > 1e-8) {
            for (let i = 0; i < embedding.length; i++) {
                embedding[i] /= magnitude;
            }
        }
    }
    // Helper methods
    hashToken(token, seed) {
        let hash = seed;
        for (let i = 0; i < token.length; i++) {
            hash = ((hash << 5) - hash + token.charCodeAt(i)) & 0x7fffffff;
        }
        return (hash / 0x7fffffff) * 2 - 1; // Normalize to [-1, 1]
    }
    countProgrammingKeywords(tokens) {
        const keywords = ['function', 'class', 'import', 'export', 'const', 'let', 'var',
            'if', 'else', 'for', 'while', 'async', 'await', 'return', 'try', 'catch'];
        return tokens.filter(token => keywords.includes(token));
    }
    extractTextualContent(text) {
        // Extract meaningful textual features beyond just code structure
        const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
        const avgSentenceLength = sentences.reduce((sum, s) => sum + s.length, 0) / Math.max(sentences.length, 1);
        return [
            sentences.length / 10,
            avgSentenceLength / 100,
            (text.match(/[A-Z][a-z]+/g) || []).length / 20, // CamelCase words
            (text.match(/[a-z_]+/g) || []).length / 50 // snake_case words
        ];
    }
    analyzeIndentation(text) {
        const lines = text.split('\n');
        const indentations = lines.map(line => {
            const match = line.match(/^(\s*)/);
            return match ? match[1].length : 0;
        });
        // Create indentation pattern signature
        const pattern = new Array(32).fill(0);
        for (let i = 0; i < Math.min(pattern.length, indentations.length); i++) {
            pattern[i] = Math.min(indentations[i] / 8, 1); // Normalize
        }
        return pattern;
    }
    estimateComplexity(text) {
        const complexityKeywords = ['if', 'for', 'while', 'switch', 'try', 'catch'];
        let complexity = 1;
        complexityKeywords.forEach(keyword => {
            const matches = text.toLowerCase().match(new RegExp(`\\b${keyword}\\b`, 'g'));
            complexity += matches ? matches.length : 0;
        });
        return Math.min(complexity, 50);
    }
    encodeKeywordSemantic(keyword) {
        // Semantic encoding of programming keywords
        const semanticMap = {
            'function': 0.9, 'class': 0.9, 'interface': 0.8,
            'import': 0.7, 'export': 0.7, 'const': 0.6,
            'async': 0.8, 'await': 0.8, 'promise': 0.8,
            'if': 0.5, 'else': 0.5, 'for': 0.6, 'while': 0.6
        };
        return semanticMap[keyword] || 0.3;
    }
    calculateContextWeight(textFeatures, index) {
        // Calculate contextual importance weights
        const baseWeight = 0.5;
        const featureInfluence = textFeatures.reduce((sum, feature, i) => {
            return sum + feature * Math.sin((index + i) * 0.1);
        }, 0);
        return baseWeight + Math.tanh(featureInfluence) * 0.3;
    }
    calculateFeatureImportance(features, layer) {
        // Calculate importance based on layer and content
        const layerWeights = [0.6, 0.8, 1.0, 0.9, 0.7, 0.8]; // Layer importance
        const baseImportance = layerWeights[layer] || 0.5;
        // Boost importance for code-related features
        let boost = 0;
        if (features.isCode)
            boost += 0.2;
        if (features.hasFunctions || features.hasClasses)
            boost += 0.3;
        if (features.vocabularyRichness > 0.5)
            boost += 0.1;
        return baseImportance + boost;
    }
    getDimensions() {
        return this.DIMENSIONS;
    }
    getModel() {
        return this.MODEL_NAME;
    }
}
exports.LocalEmbeddingProvider = LocalEmbeddingProvider;
/**
 * Content Processor - Main service implementing SOLID principles
 */
class ContentProcessor {
    config;
    chunkIdCounter = 0;
    constructor(config) {
        this.config = {
            chunkSize: 1000,
            chunkOverlap: 200,
            significanceThreshold: 0.6,
            extractComments: true,
            extractDocstrings: true,
            preserveCodeStructure: true,
            minChunkSize: 100,
            maxChunkSize: 2000,
            ...config
        };
    }
    /**
     * Process file content and generate embeddings
     */
    async processFile(file) {
        const startTime = Date.now();
        try {
            // Reduced verbosity - individual file processing not logged
            // Extract content from file
            const content = await this.extractContent(file);
            // Create content chunks
            const chunks = await this.createChunks(content, file);
            const processingTime = Date.now() - startTime;
            // Individual file completion not logged for cleaner output
            return {
                filePath: file.path,
                chunks,
                processingStats: {
                    totalChunks: chunks.length,
                    totalTokens: chunks.reduce((sum, chunk) => sum + chunk.metadata.tokens, 0),
                    processingTime,
                    chunkStrategy: this.config.preserveCodeStructure ? 'structure-aware' : 'sliding-window'
                }
            };
        }
        catch (error) {
            console.warn(`Failed to process ${file.path}: ${error.message}`);
            // Return minimal result for failed processing
            return {
                filePath: file.path,
                chunks: [],
                processingStats: {
                    totalChunks: 0,
                    totalTokens: 0,
                    processingTime: Date.now() - startTime,
                    chunkStrategy: 'failed'
                }
            };
        }
    }
    /**
     * Process multiple files efficiently
     */
    async processFiles(files) {
        const results = [];
        const batchSize = 5; // Process files in batches to manage memory
        // Content processing started - reduced verbosity
        for (let i = 0; i < files.length; i += batchSize) {
            const batch = files.slice(i, i + batchSize);
            const batchPromises = batch.map(file => this.processFile(file));
            try {
                const batchResults = await Promise.allSettled(batchPromises);
                for (const result of batchResults) {
                    if (result.status === 'fulfilled') {
                        results.push(result.value);
                    }
                }
                // Rate limiting between batches
                if (i + batchSize < files.length) {
                    await this.delay(500); // 500ms between batches
                }
            }
            catch (error) {
                console.warn(`Batch processing failed: ${error.message}`);
            }
        }
        const totalChunks = results.reduce((sum, result) => sum + result.chunks.length, 0);
        // Content processing complete - summary shown in main init flow
        return results;
    }
    /**
     * Extract content from file based on type and language
     */
    async extractContent(file) {
        const content = await fs.readFile(file.path, 'utf8');
        // Handle different file types
        switch (file.type) {
            case 'source':
                return this.processSourceContent(content, file.language);
            case 'config':
                return this.processConfigContent(content, file.extension);
            case 'documentation':
                return this.processDocumentationContent(content, file.extension);
            default:
                return content.trim();
        }
    }
    processSourceContent(content, language) {
        // For source files, we might want to extract meaningful sections
        // This is a simplified version - could be enhanced with AST parsing
        return content.trim();
    }
    processConfigContent(content, extension) {
        // For config files, extract key-value information
        return content.trim();
    }
    processDocumentationContent(content, extension) {
        // For documentation, focus on text content
        return content.trim();
    }
    /**
     * Create content chunks using configured strategy
     */
    async createChunks(content, file) {
        if (this.config.preserveCodeStructure && file.type === 'source') {
            return this.createStructureAwareChunks(content, file);
        }
        else {
            return this.createSlidingWindowChunks(content, file);
        }
    }
    async createStructureAwareChunks(content, file) {
        const chunks = [];
        const lines = content.split('\n');
        let currentChunk = '';
        let startLine = 1;
        let currentLine = 1;
        for (const line of lines) {
            currentChunk += line + '\n';
            // Check if we should create a chunk (function end, class end, etc.)
            const shouldChunk = this.shouldCreateChunk(line, currentChunk.length);
            if (shouldChunk || currentChunk.length >= this.config.maxChunkSize) {
                if (currentChunk.trim().length >= this.config.minChunkSize) {
                    chunks.push(this.createChunk(currentChunk.trim(), startLine, currentLine, file));
                }
                currentChunk = '';
                startLine = currentLine + 1;
            }
            currentLine++;
        }
        // Add remaining content as final chunk
        if (currentChunk.trim().length >= this.config.minChunkSize) {
            chunks.push(this.createChunk(currentChunk.trim(), startLine, currentLine - 1, file));
        }
        return chunks;
    }
    createSlidingWindowChunks(content, file) {
        const chunks = [];
        const lines = content.split('\n');
        for (let i = 0; i < lines.length; i += Math.floor(this.config.chunkSize / 100)) {
            const endIndex = Math.min(i + Math.floor(this.config.chunkSize / 80), lines.length);
            const chunkLines = lines.slice(i, endIndex);
            const chunkContent = chunkLines.join('\n');
            if (chunkContent.trim().length >= this.config.minChunkSize) {
                chunks.push(this.createChunk(chunkContent, i + 1, endIndex, file));
            }
        }
        return chunks;
    }
    shouldCreateChunk(line, currentLength) {
        // Simple heuristics for structure-aware chunking
        const trimmedLine = line.trim();
        return (currentLength >= this.config.chunkSize ||
            trimmedLine === '}' ||
            trimmedLine === '};' ||
            trimmedLine.startsWith('export ') ||
            trimmedLine.startsWith('class ') ||
            trimmedLine.startsWith('function ') ||
            trimmedLine.startsWith('interface ') ||
            trimmedLine.startsWith('type '));
    }
    createChunk(content, startLine, endLine, file) {
        const chunkType = this.determineChunkType(content, file);
        const tokens = this.estimateTokens(content);
        const keywords = this.extractKeywords(content, file.language);
        const significance = this.calculateSignificance(content, tokens);
        return {
            id: this.generateChunkId(),
            content,
            startLine,
            endLine,
            chunkType,
            language: file.language,
            filePath: file.path,
            metadata: {
                tokens,
                significance,
                keywords,
                imports: this.extractImports(content, file.language),
                exports: this.extractExports(content, file.language)
            }
        };
    }
    determineChunkType(content, file) {
        if (file.type === 'config')
            return 'config';
        if (file.type === 'documentation')
            return 'documentation';
        const commentRatio = (content.match(/\/\/|\/\*|\*/g) || []).length / content.split('\n').length;
        if (commentRatio > 0.5)
            return 'comment';
        return 'code';
    }
    estimateTokens(content) {
        // Simple token estimation (actual tokenizer would be more accurate)
        return Math.ceil(content.split(/\s+/).length * 1.3);
    }
    extractKeywords(content, language) {
        const keywords = new Set();
        // Extract function names, class names, important identifiers
        const matches = content.match(/\b[a-zA-Z_$][a-zA-Z0-9_$]*\b/g) || [];
        matches.forEach(match => {
            const m = String(match);
            if (m.length > 3 && !this.isCommonWord(m)) {
                keywords.add(m.toLowerCase());
            }
        });
        return Array.from(keywords).slice(0, 10); // Limit to top 10 keywords
    }
    isCommonWord(word) {
        const common = new Set(['const', 'let', 'var', 'function', 'class', 'if', 'else', 'for', 'while', 'return', 'true', 'false', 'null', 'undefined']);
        return common.has(word.toLowerCase());
    }
    extractImports(content, language) {
        const imports = [];
        const importRegex = /import\s+.*?from\s+['"`]([^'"`]+)['"`]/g;
        let match;
        while ((match = importRegex.exec(content)) !== null) {
            imports.push(match[1]);
        }
        return imports;
    }
    extractExports(content, language) {
        const exports = [];
        const exportRegex = /export\s+(?:class|function|interface|type|const|let|var)\s+([a-zA-Z_$][a-zA-Z0-9_$]*)/g;
        let match;
        while ((match = exportRegex.exec(content)) !== null) {
            exports.push(match[1]);
        }
        return exports;
    }
    calculateSignificance(content, tokens) {
        // Significance based on content characteristics
        const hasExports = content.includes('export');
        const hasClasses = content.includes('class ');
        const hasFunctions = content.includes('function ');
        const hasInterfaces = content.includes('interface ');
        const hasComments = content.includes('//') || content.includes('/*');
        const significanceScore = (hasExports ? 0.3 : 0) +
            (hasClasses ? 0.2 : 0) +
            (hasFunctions ? 0.15 : 0) +
            (hasInterfaces ? 0.15 : 0) +
            (hasComments ? 0.1 : 0) +
            (tokens > 100 ? 0.1 : 0);
        if (significanceScore >= this.config.significanceThreshold)
            return 'high';
        if (significanceScore >= 0.3)
            return 'medium';
        return 'low';
    }
    generateChunkId() {
        return `chunk_${++this.chunkIdCounter}_${Date.now()}`;
    }
    delay(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }
    /**
     * Update configuration at runtime
     */
    updateConfig(newConfig) {
        this.config = { ...this.config, ...newConfig };
    }
    /**
     * Get current configuration
     */
    getConfig() {
        return { ...this.config };
    }
}
exports.ContentProcessor = ContentProcessor;
//# sourceMappingURL=content-processor.js.map