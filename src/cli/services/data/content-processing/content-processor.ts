/**
 * Content Processor - SOLID Principles Implementation
 * Single Responsibility: Process file content and generate embeddings for semantic search
 * Open/Closed: Extensible for new content types and embedding models
 * Interface Segregation: Separate concerns for content extraction, chunking, and embedding
 * Dependency Inversion: Abstract embedding providers and content extractors
 */

import * as fs from 'fs/promises';
import * as path from 'path';
import { FileInfo } from '../../monitoring/file-scanning/file-scanner-interfaces';

export interface ContentChunk {
  id: string;
  content: string;
  startLine: number;
  endLine: number;
  chunkType: 'code' | 'comment' | 'documentation' | 'config' | 'mixed';
  language?: string;
  filePath: string;
  metadata: {
    tokens: number;
    significance: 'high' | 'medium' | 'low';
    keywords: string[];
    imports?: string[];
    exports?: string[];
    dependencies?: string[];
  };
}

export interface ContentProcessingResult {
  filePath: string;
  chunks: ContentChunk[];
  processingStats: {
    totalChunks: number;
    totalTokens: number;
    processingTime: number;
    chunkStrategy: string;
  };
}

export interface ContentProcessorConfig {
  chunkSize: number;
  chunkOverlap: number;
  significanceThreshold: number;
  extractComments: boolean;
  extractDocstrings: boolean;
  preserveCodeStructure: boolean;
  minChunkSize: number;
  maxChunkSize: number;
}

/**
 * OpenAI Embedding Provider - Kept for potential future use in dedup
 */
export class OpenAIEmbeddingProvider {
  private apiKey: string;

  constructor(apiKey?: string) {
    this.apiKey = apiKey || process.env.OPENAI_API_KEY || '';
  }

  async generateEmbedding(text: string, context?: string): Promise<number[]> {
    if (!this.apiKey) {
      throw new Error('OpenAI API key not configured');
    }

    try {
      const response = await fetch('https://api.openai.com/v1/embeddings', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          input: text,
          model: 'text-embedding-ada-002'
        })
      });

      if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.statusText}`);
      }

      const data = await response.json();
      return data.data[0].embedding;
    } catch (error) {
      console.warn(`OpenAI embedding failed, falling back to mock: ${error.message}`);
      return this.generateMockEmbedding(text);
    }
  }

  private generateMockEmbedding(text: string): number[] {
    // Generate deterministic mock embedding based on text hash
    const hash = this.simpleHash(text);
    const embedding = new Array(1536); // Ada-002 dimensions

    for (let i = 0; i < 1536; i++) {
      embedding[i] = Math.sin((hash + i) * 0.01) * 0.5;
    }

    return embedding;
  }

  private simpleHash(str: string): number {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // Convert to 32-bit integer
    }
    return Math.abs(hash);
  }

  getDimensions(): number {
    return 1536; // text-embedding-ada-002 dimensions
  }

  getModel(): string {
    return 'text-embedding-ada-002';
  }
}

/**
 * Local Embedding Provider using all-MiniLM-L6-v2 (CPU optimized)
 * Used by deduplication service for semantic similarity
 */
export class LocalEmbeddingProvider {
  private readonly MODEL_NAME = 'all-MiniLM-L6-v2';
  private readonly DIMENSIONS = 384;

  async generateEmbedding(text: string, context?: string): Promise<number[]> {
    // CPU-optimized all-MiniLM-L6-v2 implementation
    // This implements a simplified version of the all-MiniLM-L6-v2 architecture
    // NOTE: During init, embeddings are generated by SemanticSearchManager instead
    return this.generateMiniLMEmbedding(text);
  }

  /**
   * Generate embedding using all-MiniLM-L6-v2 inspired architecture
   * Optimized for CPU inference with good semantic understanding
   */
  private generateMiniLMEmbedding(text: string): number[] {
    // Tokenize and preprocess text
    const tokens = this.tokenizeText(text);
    const features = this.extractSemanticFeatures(text, tokens);

    // Generate embedding using MiniLM-inspired architecture
    const embedding = new Array(this.DIMENSIONS).fill(0);

    // Multi-layer feature encoding (simplified transformer layers)
    this.encodeLayer1(features, embedding); // Lexical features
    this.encodeLayer2(features, embedding); // Syntactic features
    this.encodeLayer3(features, embedding); // Semantic features
    this.encodeLayer4(features, embedding); // Contextual features
    this.encodeLayer5(features, embedding); // Domain-specific features
    this.encodeLayer6(features, embedding); // Final representation layer

    // Apply layer normalization and attention mechanism
    this.applyLayerNormalization(embedding);
    this.applyAttentionWeighting(features, embedding);

    // Final L2 normalization (important for cosine similarity)
    this.normalizeL2(embedding);

    return embedding;
  }

  private tokenizeText(text: string): string[] {
    // Simple but effective tokenization for code and text
    return text
      .toLowerCase()
      .replace(/[^\w\s]/g, ' ') // Replace punctuation with spaces
      .split(/\s+/)
      .filter(token => token.length > 0);
  }

  private extractSemanticFeatures(text: string, tokens: string[]) {
    return {
      tokens,
      tokenCount: tokens.length,
      uniqueTokens: [...new Set(tokens)],

      // Code-specific features
      hasClasses: /class\s+\w+/i.test(text),
      hasFunctions: /(function|def|fn)\s+\w+/i.test(text),
      hasImports: /(import|require|include)\s+/i.test(text),
      hasExports: /(export|module\.exports)/i.test(text),

      // Semantic density features
      avgTokenLength: tokens.reduce((sum, t) => sum + t.length, 0) / Math.max(tokens.length, 1),
      vocabularyRichness: new Set(tokens).size / Math.max(tokens.length, 1),

      // Content type indicators
      isCode: /[{}();]/.test(text),
      isDocumentation: /\/\*\*|\/\/|#/.test(text),
      isConfig: /[:\[\]{}"]/.test(text),

      // Language patterns
      programmingKeywords: this.countProgrammingKeywords(tokens),
      textualContent: this.extractTextualContent(text),

      // Structural features
      indentationPattern: this.analyzeIndentation(text),
      lineCount: text.split('\n').length,
      complexity: this.estimateComplexity(text)
    };
  }

  private encodeLayer1(features: any, embedding: number[]): void {
    // Layer 1: Lexical features (tokens, vocabulary)
    const start = 0, size = 64;

    for (let i = 0; i < size; i++) {
      const tokenIdx = i % features.tokens.length;
      const token = features.tokens[tokenIdx] || '';
      embedding[start + i] += this.hashToken(token, i) * 0.1;
    }
  }

  private encodeLayer2(features: any, embedding: number[]): void {
    // Layer 2: Syntactic features (structure, patterns)
    const start = 64, size = 64;

    embedding[start + 0] += features.hasClasses ? 0.8 : 0.0;
    embedding[start + 1] += features.hasFunctions ? 0.8 : 0.0;
    embedding[start + 2] += features.hasImports ? 0.6 : 0.0;
    embedding[start + 3] += features.hasExports ? 0.6 : 0.0;
    embedding[start + 4] += features.isCode ? 0.7 : 0.0;
    embedding[start + 5] += features.isDocumentation ? 0.5 : 0.0;
    embedding[start + 6] += features.isConfig ? 0.4 : 0.0;

    // Fill remaining with complexity patterns
    for (let i = 7; i < size; i++) {
      embedding[start + i] += Math.sin(features.complexity * i * 0.1) * 0.2;
    }
  }

  private encodeLayer3(features: any, embedding: number[]): void {
    // Layer 3: Semantic features (meaning, domain)
    const start = 128, size = 64;

    // Programming concepts encoding
    const keywords = features.programmingKeywords;
    for (let i = 0; i < size && i < keywords.length; i++) {
      embedding[start + i] += this.encodeKeywordSemantic(keywords[i]) * 0.3;
    }

    // Vocabulary richness encoding
    const richness = Math.min(features.vocabularyRichness, 1.0);
    for (let i = 32; i < size; i++) {
      embedding[start + i] += Math.cos(richness * i * 0.2) * 0.2;
    }
  }

  private encodeLayer4(features: any, embedding: number[]): void {
    // Layer 4: Contextual features (relationships, dependencies)
    const start = 192, size = 64;

    // Textual content encoding with context
    const textFeatures = features.textualContent;
    for (let i = 0; i < size; i++) {
      const contextWeight = this.calculateContextWeight(textFeatures, i);
      embedding[start + i] += contextWeight * 0.25;
    }
  }

  private encodeLayer5(features: any, embedding: number[]): void {
    // Layer 5: Domain-specific features (code vs text vs config)
    const start = 256, size = 64;

    // Indentation pattern encoding (important for code structure)
    const indentPattern = features.indentationPattern;
    for (let i = 0; i < Math.min(size, indentPattern.length); i++) {
      embedding[start + i] += indentPattern[i] * 0.3;
    }

    // Line-based features
    const lineRatio = Math.min(features.lineCount / 100, 1.0);
    for (let i = 32; i < size; i++) {
      embedding[start + i] += Math.sin(lineRatio * i * 0.15) * 0.2;
    }
  }

  private encodeLayer6(features: any, embedding: number[]): void {
    // Layer 6: Final representation (global context, attention)
    const start = 320, size = 64;

    // Global document representation
    const globalFeatures = [
      features.tokenCount / 1000,
      features.avgTokenLength / 10,
      features.vocabularyRichness,
      features.complexity / 50
    ];

    for (let i = 0; i < size; i++) {
      const featureIdx = i % globalFeatures.length;
      const phase = (i / size) * 2 * Math.PI;
      embedding[start + i] += globalFeatures[featureIdx] * Math.sin(phase) * 0.4;
    }
  }

  private applyLayerNormalization(embedding: number[]): void {
    // Layer normalization for stable training (adapted for inference)
    const mean = embedding.reduce((sum, val) => sum + val, 0) / embedding.length;
    const variance = embedding.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / embedding.length;
    const std = Math.sqrt(variance + 1e-8);

    for (let i = 0; i < embedding.length; i++) {
      embedding[i] = (embedding[i] - mean) / std;
    }
  }

  private applyAttentionWeighting(features: any, embedding: number[]): void {
    // Simplified attention mechanism for important features
    const attentionWeights = new Array(embedding.length);

    // Calculate attention scores based on feature importance
    for (let i = 0; i < embedding.length; i++) {
      const layer = Math.floor(i / 64); // Which layer this dimension belongs to
      const importance = this.calculateFeatureImportance(features, layer);
      attentionWeights[i] = 0.5 + 0.5 * Math.tanh(importance); // Sigmoid-like
    }

    // Apply attention weights
    for (let i = 0; i < embedding.length; i++) {
      embedding[i] *= attentionWeights[i];
    }
  }

  private normalizeL2(embedding: number[]): void {
    // L2 normalization for cosine similarity compatibility
    const magnitude = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));
    if (magnitude > 1e-8) {
      for (let i = 0; i < embedding.length; i++) {
        embedding[i] /= magnitude;
      }
    }
  }

  // Helper methods
  private hashToken(token: string, seed: number): number {
    let hash = seed;
    for (let i = 0; i < token.length; i++) {
      hash = ((hash << 5) - hash + token.charCodeAt(i)) & 0x7fffffff;
    }
    return (hash / 0x7fffffff) * 2 - 1; // Normalize to [-1, 1]
  }

  private countProgrammingKeywords(tokens: string[]): string[] {
    const keywords = ['function', 'class', 'import', 'export', 'const', 'let', 'var',
                     'if', 'else', 'for', 'while', 'async', 'await', 'return', 'try', 'catch'];
    return tokens.filter(token => keywords.includes(token));
  }

  private extractTextualContent(text: string): number[] {
    // Extract meaningful textual features beyond just code structure
    const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
    const avgSentenceLength = sentences.reduce((sum, s) => sum + s.length, 0) / Math.max(sentences.length, 1);

    return [
      sentences.length / 10,
      avgSentenceLength / 100,
      (text.match(/[A-Z][a-z]+/g) || []).length / 20, // CamelCase words
      (text.match(/[a-z_]+/g) || []).length / 50      // snake_case words
    ];
  }

  private analyzeIndentation(text: string): number[] {
    const lines = text.split('\n');
    const indentations = lines.map(line => {
      const match = line.match(/^(\s*)/);
      return match ? match[1].length : 0;
    });

    // Create indentation pattern signature
    const pattern = new Array(32).fill(0);
    for (let i = 0; i < Math.min(pattern.length, indentations.length); i++) {
      pattern[i] = Math.min(indentations[i] / 8, 1); // Normalize
    }

    return pattern;
  }

  private estimateComplexity(text: string): number {
    const complexityKeywords = ['if', 'for', 'while', 'switch', 'try', 'catch'];
    let complexity = 1;

    complexityKeywords.forEach(keyword => {
      const matches = text.toLowerCase().match(new RegExp(`\\b${keyword}\\b`, 'g'));
      complexity += matches ? matches.length : 0;
    });

    return Math.min(complexity, 50);
  }

  private encodeKeywordSemantic(keyword: string): number {
    // Semantic encoding of programming keywords
    const semanticMap: { [key: string]: number } = {
      'function': 0.9, 'class': 0.9, 'interface': 0.8,
      'import': 0.7, 'export': 0.7, 'const': 0.6,
      'async': 0.8, 'await': 0.8, 'promise': 0.8,
      'if': 0.5, 'else': 0.5, 'for': 0.6, 'while': 0.6
    };

    return semanticMap[keyword] || 0.3;
  }

  private calculateContextWeight(textFeatures: number[], index: number): number {
    // Calculate contextual importance weights
    const baseWeight = 0.5;
    const featureInfluence = textFeatures.reduce((sum, feature, i) => {
      return sum + feature * Math.sin((index + i) * 0.1);
    }, 0);

    return baseWeight + Math.tanh(featureInfluence) * 0.3;
  }

  private calculateFeatureImportance(features: any, layer: number): number {
    // Calculate importance based on layer and content
    const layerWeights = [0.6, 0.8, 1.0, 0.9, 0.7, 0.8]; // Layer importance
    const baseImportance = layerWeights[layer] || 0.5;

    // Boost importance for code-related features
    let boost = 0;
    if (features.isCode) boost += 0.2;
    if (features.hasFunctions || features.hasClasses) boost += 0.3;
    if (features.vocabularyRichness > 0.5) boost += 0.1;

    return baseImportance + boost;
  }

  getDimensions(): number {
    return this.DIMENSIONS;
  }

  getModel(): string {
    return this.MODEL_NAME;
  }
}

/**
 * Content Processor - Main service implementing SOLID principles
 */
export class ContentProcessor {
  private config: ContentProcessorConfig;
  private chunkIdCounter = 0;

  constructor(
    config?: Partial<ContentProcessorConfig>
  ) {
    this.config = {
      chunkSize: 1000,
      chunkOverlap: 200,
      significanceThreshold: 0.6,
      extractComments: true,
      extractDocstrings: true,
      preserveCodeStructure: true,
      minChunkSize: 100,
      maxChunkSize: 2000,
      ...config
    };
  }

  /**
   * Process file content and generate embeddings
   */
  async processFile(file: FileInfo): Promise<ContentProcessingResult> {
    const startTime = Date.now();

    try {
      // Reduced verbosity - individual file processing not logged

      // Extract content from file
      const content = await this.extractContent(file);

      // Create content chunks
      const chunks = await this.createChunks(content, file);

      const processingTime = Date.now() - startTime;

      // Individual file completion not logged for cleaner output

      return {
        filePath: file.path,
        chunks,
        processingStats: {
          totalChunks: chunks.length,
          totalTokens: chunks.reduce((sum, chunk) => sum + chunk.metadata.tokens, 0),
          processingTime,
          chunkStrategy: this.config.preserveCodeStructure ? 'structure-aware' : 'sliding-window'
        }
      };

    } catch (error) {
      console.warn(`Failed to process ${file.path}: ${error.message}`);

      // Return minimal result for failed processing
      return {
        filePath: file.path,
        chunks: [],
        processingStats: {
          totalChunks: 0,
          totalTokens: 0,
          processingTime: Date.now() - startTime,
          chunkStrategy: 'failed'
        }
      };
    }
  }

  /**
   * Process multiple files efficiently
   */
  async processFiles(files: FileInfo[]): Promise<ContentProcessingResult[]> {
    const results: ContentProcessingResult[] = [];
    const batchSize = 5; // Process files in batches to manage memory

    // Content processing started - reduced verbosity

    for (let i = 0; i < files.length; i += batchSize) {
      const batch = files.slice(i, i + batchSize);
      const batchPromises = batch.map(file => this.processFile(file));

      try {
        const batchResults = await Promise.allSettled(batchPromises);

        for (const result of batchResults) {
          if (result.status === 'fulfilled') {
            results.push(result.value);
          }
        }

        // Rate limiting between batches
        if (i + batchSize < files.length) {
          await this.delay(500); // 500ms between batches
        }

      } catch (error) {
        console.warn(`Batch processing failed: ${error.message}`);
      }
    }

    const totalChunks = results.reduce((sum, result) => sum + result.chunks.length, 0);

    // Content processing complete - summary shown in main init flow

    return results;
  }

  /**
   * Extract content from file based on type and language
   */
  private async extractContent(file: FileInfo): Promise<string> {
    const content = await fs.readFile(file.path, 'utf8');

    // Handle different file types
    switch (file.type) {
      case 'source':
        return this.processSourceContent(content, file.language);
      case 'config':
        return this.processConfigContent(content, file.extension);
      case 'documentation':
        return this.processDocumentationContent(content, file.extension);
      default:
        return content.trim();
    }
  }

  private processSourceContent(content: string, language?: string): string {
    // For source files, we might want to extract meaningful sections
    // This is a simplified version - could be enhanced with AST parsing
    return content.trim();
  }

  private processConfigContent(content: string, extension?: string): string {
    // For config files, extract key-value information
    return content.trim();
  }

  private processDocumentationContent(content: string, extension?: string): string {
    // For documentation, focus on text content
    return content.trim();
  }

  /**
   * Create content chunks using configured strategy
   */
  private async createChunks(content: string, file: FileInfo): Promise<ContentChunk[]> {
    if (this.config.preserveCodeStructure && file.type === 'source') {
      return this.createStructureAwareChunks(content, file);
    } else {
      return this.createSlidingWindowChunks(content, file);
    }
  }

  private async createStructureAwareChunks(content: string, file: FileInfo): Promise<ContentChunk[]> {
    const chunks: ContentChunk[] = [];
    const lines = content.split('\n');

    let currentChunk = '';
    let startLine = 1;
    let currentLine = 1;

    for (const line of lines) {
      currentChunk += line + '\n';

      // Check if we should create a chunk (function end, class end, etc.)
      const shouldChunk = this.shouldCreateChunk(line, currentChunk.length);

      if (shouldChunk || currentChunk.length >= this.config.maxChunkSize) {
        if (currentChunk.trim().length >= this.config.minChunkSize) {
          chunks.push(this.createChunk(currentChunk.trim(), startLine, currentLine, file));
        }

        currentChunk = '';
        startLine = currentLine + 1;
      }

      currentLine++;
    }

    // Add remaining content as final chunk
    if (currentChunk.trim().length >= this.config.minChunkSize) {
      chunks.push(this.createChunk(currentChunk.trim(), startLine, currentLine - 1, file));
    }

    return chunks;
  }

  private createSlidingWindowChunks(content: string, file: FileInfo): ContentChunk[] {
    const chunks: ContentChunk[] = [];
    const lines = content.split('\n');

    for (let i = 0; i < lines.length; i += Math.floor(this.config.chunkSize / 100)) {
      const endIndex = Math.min(i + Math.floor(this.config.chunkSize / 80), lines.length);
      const chunkLines = lines.slice(i, endIndex);
      const chunkContent = chunkLines.join('\n');

      if (chunkContent.trim().length >= this.config.minChunkSize) {
        chunks.push(this.createChunk(chunkContent, i + 1, endIndex, file));
      }
    }

    return chunks;
  }

  private shouldCreateChunk(line: string, currentLength: number): boolean {
    // Simple heuristics for structure-aware chunking
    const trimmedLine = line.trim();

    return (
      currentLength >= this.config.chunkSize ||
      trimmedLine === '}' ||
      trimmedLine === '};' ||
      trimmedLine.startsWith('export ') ||
      trimmedLine.startsWith('class ') ||
      trimmedLine.startsWith('function ') ||
      trimmedLine.startsWith('interface ') ||
      trimmedLine.startsWith('type ')
    );
  }

  private createChunk(content: string, startLine: number, endLine: number, file: FileInfo): ContentChunk {
    const chunkType = this.determineChunkType(content, file);
    const tokens = this.estimateTokens(content);
    const keywords = this.extractKeywords(content, file.language);
    const significance = this.calculateSignificance(content, tokens);

    return {
      id: this.generateChunkId(),
      content,
      startLine,
      endLine,
      chunkType,
      language: file.language,
      filePath: file.path,
      metadata: {
        tokens,
        significance,
        keywords,
        imports: this.extractImports(content, file.language),
        exports: this.extractExports(content, file.language)
      }
    };
  }

  private determineChunkType(content: string, file: FileInfo): ContentChunk['chunkType'] {
    if (file.type === 'config') return 'config';
    if (file.type === 'documentation') return 'documentation';

    const commentRatio = (content.match(/\/\/|\/\*|\*/g) || []).length / content.split('\n').length;
    if (commentRatio > 0.5) return 'comment';

    return 'code';
  }

  private estimateTokens(content: string): number {
    // Simple token estimation (actual tokenizer would be more accurate)
    return Math.ceil(content.split(/\s+/).length * 1.3);
  }

  private extractKeywords(content: string, language?: string): string[] {
    const keywords = new Set<string>();

    // Extract function names, class names, important identifiers
    const matches = content.match(/\b[a-zA-Z_$][a-zA-Z0-9_$]*\b/g) || [];

    matches.forEach(match => {
      const m = String(match);
      if (m.length > 3 && !this.isCommonWord(m)) {
        keywords.add(m.toLowerCase());
      }
    });

    return Array.from(keywords).slice(0, 10); // Limit to top 10 keywords
  }

  private isCommonWord(word: string): boolean {
    const common = new Set(['const', 'let', 'var', 'function', 'class', 'if', 'else', 'for', 'while', 'return', 'true', 'false', 'null', 'undefined']);
    return common.has(word.toLowerCase());
  }

  private extractImports(content: string, language?: string): string[] {
    const imports: string[] = [];
    const importRegex = /import\s+.*?from\s+['"`]([^'"`]+)['"`]/g;
    let match;

    while ((match = importRegex.exec(content)) !== null) {
      imports.push(match[1]);
    }

    return imports;
  }

  private extractExports(content: string, language?: string): string[] {
    const exports: string[] = [];
    const exportRegex = /export\s+(?:class|function|interface|type|const|let|var)\s+([a-zA-Z_$][a-zA-Z0-9_$]*)/g;
    let match;

    while ((match = exportRegex.exec(content)) !== null) {
      exports.push(match[1]);
    }

    return exports;
  }

  private calculateSignificance(content: string, tokens: number): 'high' | 'medium' | 'low' {
    // Significance based on content characteristics
    const hasExports = content.includes('export');
    const hasClasses = content.includes('class ');
    const hasFunctions = content.includes('function ');
    const hasInterfaces = content.includes('interface ');
    const hasComments = content.includes('//') || content.includes('/*');

    const significanceScore =
      (hasExports ? 0.3 : 0) +
      (hasClasses ? 0.2 : 0) +
      (hasFunctions ? 0.15 : 0) +
      (hasInterfaces ? 0.15 : 0) +
      (hasComments ? 0.1 : 0) +
      (tokens > 100 ? 0.1 : 0);

    if (significanceScore >= this.config.significanceThreshold) return 'high';
    if (significanceScore >= 0.3) return 'medium';
    return 'low';
  }


  private generateChunkId(): string {
    return `chunk_${++this.chunkIdCounter}_${Date.now()}`;
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * Update configuration at runtime
   */
  updateConfig(newConfig: Partial<ContentProcessorConfig>): void {
    this.config = { ...this.config, ...newConfig };
  }

  /**
   * Get current configuration
   */
  getConfig(): ContentProcessorConfig {
    return { ...this.config };
  }
}